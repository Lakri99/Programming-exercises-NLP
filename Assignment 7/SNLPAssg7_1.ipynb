{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"SNLPAssg7_1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"cdgjBf3E7Xk9","colab_type":"code","colab":{}},"source":["import math\n","import re\n","import operator\n","\n","from collections import Counter\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eovBA1bd7XlM","colab_type":"code","colab":{}},"source":["def tokenize(text):\n","    #\"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n","    return re.findall('[a-z]+', text.lower())\n","\n","def find_bigrams(text):\n","    bi_dict = {}\n","    for i in range(0, len(text) - 1):\n","        (first, second) = (text[i], text[i+1])\n","        if not (first, second) in bi_dict:\n","            bi_dict[(first, second)] = 1\n","        else:\n","            bi_dict[(first, second)] += 1\n","    return bi_dict\n","\n","def find_unigrams(text):\n","    return Counter(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q84OJZDw7Xld","colab_type":"code","colab":{}},"source":["def get_params(d, bigrams, unigrams, vocab):\n","    \n","    #params to return\n","    N1_plus_wi1 = dict() #eqn 10\n","    N_bigrams = dict() #denominator of 7, 9\n","    lambda_wi = dict() #eqn 9\n","    P_abs_wi = dict() #eqn 8\n","    \n","    P_unif = 1/len(vocab)\n","    N_unis = sum(unigrams.values())\n","    N1_plus = len(unigrams)\n","    \n","    lambdadot = (d/N_unis) * N1_plus\n","    \n","    for word in vocab:\n","        count = 0 #unique bigrams starting with word\n","        N = 0 #total number of bigrams starting with word\n","        \n","        for key, val in bigrams.items():\n","            if(key[0] == word):\n","                if(val > 0):\n","                    count += 1\n","                    N += val\n","        \n","        N1_plus_wi1[word] = count \n","        N_bigrams[word] = N\n","        \n","        if N > 0:\n","            lambdad = (d/N) * N1_plus_wi1[word]\n","        else:\n","            lambdad = 0\n","            \n","        lambda_wi[word] = lambdad\n","        P_abs_wi[word] = (max((unigrams[word] - d), 0)/N_unis) + (lambdadot * P_unif)\n","          \n","    return P_unif, lambdadot, N1_plus_wi1, N_bigrams, lambda_wi, P_abs_wi\n","\n","\n","def get_P_abs_wi1(wi, wi_1, d, unigrams, bigrams, N_bigrams, lambda_wi, P_abs_wi, lambdadot, P_unif):\n","    try:\n","        if((wi_1, wi) not in bigrams.keys()):\n","            if(wi_1 not in lambda_wi.keys()):\n","                if(wi not in P_abs_wi.keys()):\n","                    P_abs_wi1 = lambdadot * P_unif\n","                else:\n","                    P_abs_wi1 = P_abs_wi[wi]\n","            else:\n","                P_abs_wi1 = (lambda_wi[wi_1] * P_abs_wi[wi])\n","        else:\n","            N = N_bigrams[wi_1]\n","            P_abs_wi1 = (max((bigrams[(wi_1, wi)] - d), 0)/N) + (lambda_wi[wi_1] * P_abs_wi[wi])\n","    except:\n","        P_abs_wi1 = lambdadot * P_unif\n","    return P_abs_wi1\n","\n","def calculate_probabilities(unigrams, bigrams, bgrams, vocab):\n","    bi_dict = {}\n","    P_unif, lambdadot, N1_plus_wi1, N_bigrams, lambda_wi, P_abs_wi = get_params(0.7, bigrams, unigrams, vocab)\n","    for key, val in bgrams.items():\n","        bi_dict[key] = get_P_abs_wi1(key[1], key[0], 0.7, unigrams, bigrams, N_bigrams, lambda_wi, P_abs_wi, lambdadot, P_unif)\n","    \n","    return P_abs_wi, bi_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_dPDFrA7Xlp","colab_type":"code","colab":{}},"source":["def prune(uni_probs, bi_probs, epsilon, ugrams, bi_grams, bgrams, vocab):\n","    \n","    unigrams = ugrams.copy()\n","    bigrams = bi_grams.copy()\n","    \n","    for key in uni_probs.keys():\n","        if(uni_probs[key] < epsilon):\n","            del unigrams[key]\n","            uni_probs\n","    for key in bi_probs.keys():\n","        if((bi_probs[key] < epsilon) and (key in bigrams)):\n","            del bigrams[key]\n","    #recalculate probabilities based on pruned ngrams            \n","    uni_dict, bi_dict = calculate_probabilities(unigrams, bigrams, bgrams, vocab)\n","    \n","    return uni_dict, bi_dict, unigrams, bigrams"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cQoIl6U7Xl0","colab_type":"code","colab":{}},"source":["def find_perplexity(bgrams, bi_dict):\n","    tsum = 0\n","    s = sum(bgrams.values())\n","    \n","    for k,v in bgrams.items():\n","        rel_freq = v / s\n","        cond_prob = bi_dict[k]\n","        tsum -= rel_freq * math.log(cond_prob)\n","    \n","    perplexity = math.exp(tsum)\n","    return perplexity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkSRCM4p7Xl9","colab_type":"code","colab":{}},"source":["trainfile = './ex7_materials/English_train.txt'\n","with open(trainfile, encoding=\"utf-8\") as f:\n","    traintext = f.read()\n","f.close()\n","\n","testfile = './ex7_materials/English_test.txt'\n","with open(testfile, encoding=\"utf-8\") as f:\n","    testtext = f.read()\n","f.close()\n","\n","train_text = tokenize(traintext)\n","test_text = tokenize(testtext)\n","\n","uni_train = find_unigrams(train_text)\n","bi_train = find_bigrams(train_text)\n","\n","vocab = list(uni_train.keys())\n","V = len(vocab)\n","\n","uni_test = find_unigrams(test_text)\n","bi_test = find_bigrams(test_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Qvw_sOb7XmG","colab_type":"code","colab":{},"outputId":"0f9b962c-b571-46e5-d3a6-f183d3150360"},"source":["uni_dict, bi_dict = calculate_probabilities(uni_train, bi_train, bi_test, vocab)\n","print(f\"Prepruning lengths: unigrams - {len(uni_train)}, bigrams - {len(bi_train)}\")\n","pp = find_perplexity(bi_test, bi_dict)\n","print(f\"Perplexity without pruning = {pp}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Prepruning lengths: unigrams - 5765, bigrams - 45533\n","Perplexity without pruning = 225.61367810627084\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gaMW_AB_7XmS","colab_type":"code","colab":{},"outputId":"e591ce3a-fa17-41c5-a469-4516291d88aa"},"source":["for i in range(2,7):\n","    epsilon = math.pow(10, i*-1)\n","    udict, bdict, unis, bis = prune(uni_dict, bi_dict, epsilon, uni_train, bi_train, bi_test, vocab)\n","    pp = find_perplexity(bi_test, bdict)\n","    print(f\"epsilon = {epsilon}, unigrams length = {len(unis)}, bigrams length = {len(bis)}, perplexity = {pp}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epsilon = 0.01, unigrams length = 15, bigrams length = 41884, perplexity = 28087.90931123331\n","epsilon = 0.001, unigrams length = 132, bigrams length = 44465, perplexity = 887.5127072455467\n","epsilon = 0.0001, unigrams length = 1014, bigrams length = 45479, perplexity = 285.6268150247238\n","epsilon = 1e-05, unigrams length = 5765, bigrams length = 45533, perplexity = 225.61367810627084\n","epsilon = 1e-06, unigrams length = 5765, bigrams length = 45533, perplexity = 225.61367810627084\n"],"name":"stdout"}]}]}